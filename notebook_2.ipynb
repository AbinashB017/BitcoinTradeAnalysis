{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97b37785",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import advanced modeling libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Time series analysis\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Advanced modeling libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f1372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from notebook_1\n",
    "# Note: In practice, you would run notebook_1 first or save/load the processed data\n",
    "\n",
    "print(\"Loading and preprocessing data for advanced modeling...\")\n",
    "\n",
    "# Load raw data\n",
    "sentiment_df = pd.read_csv('csv_files/fear_greed_index.csv')\n",
    "trading_df = pd.read_csv('csv_files/historical_data.csv')\n",
    "\n",
    "# Quick preprocessing (similar to notebook_1)\n",
    "sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
    "trading_df['date'] = pd.to_datetime(trading_df['Timestamp IST'], format='%d-%m-%Y %H:%M', errors='coerce')\n",
    "\n",
    "# Clean numeric columns\n",
    "numeric_cols = ['Execution Price', 'Size Tokens', 'Size USD', 'Start Position', 'Closed PnL', 'Fee']\n",
    "for col in numeric_cols:\n",
    "    trading_df[col] = pd.to_numeric(trading_df[col], errors='coerce')\n",
    "\n",
    "# Remove missing values\n",
    "trading_clean = trading_df.dropna(subset=['date', 'Closed PnL', 'Size USD'])\n",
    "trading_clean['trade_date'] = trading_clean['date'].dt.date\n",
    "\n",
    "# Create daily aggregations\n",
    "daily_trading = trading_clean.groupby('trade_date').agg({\n",
    "    'Closed PnL': ['sum', 'mean', 'count', 'std'],\n",
    "    'Size USD': ['sum', 'mean'],\n",
    "    'Account': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "daily_trading.columns = ['date', 'total_pnl', 'avg_pnl', 'total_trades', 'pnl_volatility', \n",
    "                        'total_volume', 'avg_trade_size', 'unique_traders']\n",
    "\n",
    "# Add win rate\n",
    "daily_win_rates = trading_clean.groupby('trade_date').apply(\n",
    "    lambda x: (x['Closed PnL'] > 0).mean()\n",
    ").reset_index()\n",
    "daily_win_rates.columns = ['date', 'daily_win_rate']\n",
    "\n",
    "daily_trading = daily_trading.merge(daily_win_rates, on='date')\n",
    "\n",
    "# Merge with sentiment\n",
    "sentiment_clean = sentiment_df.copy()\n",
    "sentiment_clean['date'] = sentiment_clean['date'].dt.date\n",
    "sentiment_clean = sentiment_clean[['date', 'value', 'classification']].drop_duplicates()\n",
    "\n",
    "# Final merged dataset\n",
    "modeling_data = daily_trading.merge(sentiment_clean, on='date', how='inner')\n",
    "\n",
    "print(f\"Modeling dataset ready: {len(modeling_data)} days of data\")\n",
    "print(f\"Features: {modeling_data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c232899f",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering for Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a90ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced feature engineering\n",
    "print(\"Creating advanced features for modeling...\")\n",
    "\n",
    "# Sort by date\n",
    "modeling_data = modeling_data.sort_values('date')\n",
    "\n",
    "# 1. Lagged features\n",
    "modeling_data['sentiment_lag1'] = modeling_data['value'].shift(1)\n",
    "modeling_data['sentiment_lag2'] = modeling_data['value'].shift(2)\n",
    "modeling_data['sentiment_lag3'] = modeling_data['value'].shift(3)\n",
    "\n",
    "modeling_data['pnl_lag1'] = modeling_data['total_pnl'].shift(1)\n",
    "modeling_data['pnl_lag2'] = modeling_data['total_pnl'].shift(2)\n",
    "\n",
    "# 2. Moving averages\n",
    "modeling_data['sentiment_ma3'] = modeling_data['value'].rolling(window=3).mean()\n",
    "modeling_data['sentiment_ma7'] = modeling_data['value'].rolling(window=7).mean()\n",
    "modeling_data['pnl_ma3'] = modeling_data['total_pnl'].rolling(window=3).mean()\n",
    "modeling_data['pnl_ma7'] = modeling_data['total_pnl'].rolling(window=7).mean()\n",
    "\n",
    "# 3. Volatility measures\n",
    "modeling_data['sentiment_volatility'] = modeling_data['value'].rolling(window=7).std()\n",
    "modeling_data['pnl_rolling_std'] = modeling_data['total_pnl'].rolling(window=7).std()\n",
    "\n",
    "# 4. Trend indicators\n",
    "modeling_data['sentiment_trend'] = modeling_data['value'].diff()\n",
    "modeling_data['sentiment_momentum'] = modeling_data['value'] - modeling_data['sentiment_ma7']\n",
    "\n",
    "# 5. Interaction features\n",
    "modeling_data['sentiment_volume_interaction'] = modeling_data['value'] * modeling_data['total_volume']\n",
    "modeling_data['sentiment_trades_interaction'] = modeling_data['value'] * modeling_data['total_trades']\n",
    "\n",
    "# 6. Categorical features\n",
    "def categorize_sentiment_detailed(value):\n",
    "    if value >= 80: return 'Extreme_Greed'\n",
    "    elif value >= 60: return 'Greed'\n",
    "    elif value >= 40: return 'Neutral'\n",
    "    elif value >= 20: return 'Fear'\n",
    "    else: return 'Extreme_Fear'\n",
    "\n",
    "modeling_data['sentiment_category'] = modeling_data['value'].apply(categorize_sentiment_detailed)\n",
    "\n",
    "# 7. Time-based features\n",
    "modeling_data['date_dt'] = pd.to_datetime(modeling_data['date'])\n",
    "modeling_data['day_of_week'] = modeling_data['date_dt'].dt.dayofweek\n",
    "modeling_data['month'] = modeling_data['date_dt'].dt.month\n",
    "modeling_data['quarter'] = modeling_data['date_dt'].dt.quarter\n",
    "\n",
    "# 8. Target variables for different prediction tasks\n",
    "modeling_data['next_day_pnl'] = modeling_data['total_pnl'].shift(-1)  # Tomorrow's PnL\n",
    "modeling_data['next_day_positive'] = (modeling_data['next_day_pnl'] > 0).astype(int)  # Binary target\n",
    "modeling_data['pnl_direction'] = np.where(modeling_data['next_day_pnl'] > modeling_data['total_pnl'], 1, 0)\n",
    "\n",
    "# Remove rows with NaN values created by feature engineering\n",
    "feature_data = modeling_data.dropna()\n",
    "\n",
    "print(f\"Feature engineering complete. Dataset shape: {feature_data.shape}\")\n",
    "print(f\"New features created: {len(feature_data.columns) - len(modeling_data.columns) + len(['sentiment_lag1', 'sentiment_lag2', 'sentiment_lag3', 'pnl_lag1', 'pnl_lag2'])}\")\n",
    "\n",
    "# Display feature correlation with target\n",
    "feature_cols = ['value', 'sentiment_lag1', 'sentiment_lag2', 'sentiment_ma3', 'sentiment_ma7', \n",
    "               'sentiment_volatility', 'sentiment_trend', 'sentiment_momentum', \n",
    "               'total_volume', 'total_trades', 'daily_win_rate']\n",
    "\n",
    "correlations = feature_data[feature_cols + ['next_day_pnl']].corr()['next_day_pnl'].abs().sort_values(ascending=False)\n",
    "print(\"\\nFeature correlations with next day PnL:\")\n",
    "print(correlations.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f02b9",
   "metadata": {},
   "source": [
    "## 3. Predictive Modeling: PnL Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dfc7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build predictive models for PnL forecasting\n",
    "print(\"=== PREDICTIVE MODELING: PNL FORECASTING ===\")\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = [\n",
    "    'value', 'sentiment_lag1', 'sentiment_lag2', 'sentiment_ma3', 'sentiment_ma7',\n",
    "    'sentiment_volatility', 'sentiment_trend', 'sentiment_momentum',\n",
    "    'total_volume', 'total_trades', 'daily_win_rate', 'avg_trade_size',\n",
    "    'pnl_lag1', 'pnl_lag2', 'pnl_ma3', 'day_of_week', 'month'\n",
    "]\n",
    "\n",
    "X = feature_data[feature_columns].fillna(0)\n",
    "y = feature_data['next_day_pnl'].fillna(0)\n",
    "\n",
    "# Split data (time series split - no shuffling)\n",
    "split_point = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]\n",
    "y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'SVR': SVR(kernel='rbf')\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    if name == 'SVR':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "\n",
    "# Feature importance (Random Forest)\n",
    "rf_model = models['Random Forest']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (Random Forest):\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05fadf0",
   "metadata": {},
   "source": [
    "## 4. Classification Model: Predicting Profitable Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification: Predicting profitable vs unprofitable days\n",
    "print(\"=== CLASSIFICATION MODEL: PREDICTING PROFITABLE DAYS ===\")\n",
    "\n",
    "# Prepare binary target\n",
    "y_binary = feature_data['next_day_positive'].fillna(0)\n",
    "\n",
    "# Split for classification\n",
    "y_train_binary, y_test_binary = y_binary.iloc[:split_point], y_binary.iloc[split_point:]\n",
    "\n",
    "# Classification models\n",
    "clf_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest Classifier': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "clf_results = {}\n",
    "\n",
    "for name, model in clf_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    if 'Logistic' in name:\n",
    "        model.fit(X_train_scaled, y_train_binary)\n",
    "        y_pred_binary = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train_binary)\n",
    "        y_pred_binary = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (y_pred_binary == y_test_binary).mean()\n",
    "    \n",
    "    clf_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred_binary,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_binary, y_pred_binary))\n",
    "\n",
    "# Feature importance for classification\n",
    "rf_clf = clf_models['Random Forest Classifier']\n",
    "clf_feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': rf_clf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Features for Predicting Profitable Days:\")\n",
    "print(clf_feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98631ffa",
   "metadata": {},
   "source": [
    "## 5. Time Series Analysis and ARIMA Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis of sentiment and PnL\n",
    "print(\"=== TIME SERIES ANALYSIS ===\")\n",
    "\n",
    "# Prepare time series data\n",
    "ts_data = feature_data.set_index('date_dt').sort_index()\n",
    "\n",
    "# 1. Seasonal decomposition of sentiment\n",
    "print(\"\\n1. SEASONAL DECOMPOSITION\")\n",
    "\n",
    "# Decompose sentiment time series\n",
    "try:\n",
    "    decomposition_sentiment = seasonal_decompose(\n",
    "        ts_data['value'].fillna(method='ffill'), \n",
    "        model='additive', \n",
    "        period=30  # Monthly seasonality\n",
    "    )\n",
    "    \n",
    "    # Plot decomposition\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "    \n",
    "    decomposition_sentiment.observed.plot(ax=axes[0], title='Original Sentiment')\n",
    "    decomposition_sentiment.trend.plot(ax=axes[1], title='Trend')\n",
    "    decomposition_sentiment.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "    decomposition_sentiment.resid.plot(ax=axes[3], title='Residual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/time_series_decomposition.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Seasonal decomposition completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Seasonal decomposition failed: {e}\")\n",
    "\n",
    "# 2. ARIMA modeling for sentiment forecasting\n",
    "print(\"\\n2. ARIMA MODELING\")\n",
    "\n",
    "# Prepare sentiment data for ARIMA\n",
    "sentiment_ts = ts_data['value'].fillna(method='ffill').dropna()\n",
    "\n",
    "# Check stationarity\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def check_stationarity(timeseries, title):\n",
    "    result = adfuller(timeseries, autolag='AIC')\n",
    "    print(f'\\n{title}:')\n",
    "    print(f'ADF Statistic: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'\\t{key}: {value}')\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Result: Time series is stationary\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Result: Time series is non-stationary\")\n",
    "        return False\n",
    "\n",
    "is_stationary = check_stationarity(sentiment_ts, \"Sentiment Time Series Stationarity Test\")\n",
    "\n",
    "# If not stationary, difference the series\n",
    "if not is_stationary:\n",
    "    sentiment_diff = sentiment_ts.diff().dropna()\n",
    "    is_stationary_diff = check_stationarity(sentiment_diff, \"Differenced Sentiment Series\")\n",
    "    \n",
    "# Fit ARIMA model\n",
    "try:\n",
    "    # Simple ARIMA(1,1,1) model\n",
    "    arima_model = ARIMA(sentiment_ts, order=(1,1,1))\n",
    "    arima_fitted = arima_model.fit()\n",
    "    \n",
    "    print(\"\\nARIMA Model Summary:\")\n",
    "    print(arima_fitted.summary())\n",
    "    \n",
    "    # Forecast next 7 days\n",
    "    forecast_steps = 7\n",
    "    forecast = arima_fitted.forecast(steps=forecast_steps)\n",
    "    \n",
    "    print(f\"\\nNext {forecast_steps} days sentiment forecast:\")\n",
    "    for i, value in enumerate(forecast, 1):\n",
    "        print(f\"Day +{i}: {value:.2f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ARIMA modeling failed: {e}\")\n",
    "\n",
    "print(\"\\nTime series analysis completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63688925",
   "metadata": {},
   "source": [
    "## 6. Advanced Pattern Recognition: Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a475b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced clustering analysis to identify market regimes\n",
    "print(\"=== ADVANCED CLUSTERING ANALYSIS ===\")\n",
    "\n",
    "# Prepare clustering features\n",
    "clustering_features = [\n",
    "    'value', 'sentiment_volatility', 'sentiment_trend',\n",
    "    'total_pnl', 'daily_win_rate', 'total_volume', 'total_trades'\n",
    "]\n",
    "\n",
    "cluster_data = feature_data[clustering_features].fillna(0)\n",
    "cluster_data_scaled = StandardScaler().fit_transform(cluster_data)\n",
    "\n",
    "# 1. K-Means Clustering for Market Regimes\n",
    "print(\"\\n1. K-MEANS CLUSTERING FOR MARKET REGIMES\")\n",
    "\n",
    "# Determine optimal number of clusters using elbow method\n",
    "inertias = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(cluster_data_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertias, marker='o')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True)\n",
    "plt.savefig('outputs/elbow_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Use optimal number of clusters (e.g., 4)\n",
    "optimal_k = 4\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "cluster_labels = kmeans_final.fit_predict(cluster_data_scaled)\n",
    "\n",
    "# Add cluster labels to data\n",
    "feature_data['market_regime'] = cluster_labels\n",
    "\n",
    "# Analyze clusters\n",
    "cluster_analysis = feature_data.groupby('market_regime')[clustering_features].mean().round(4)\n",
    "cluster_counts = feature_data['market_regime'].value_counts().sort_index()\n",
    "\n",
    "print(f\"\\nIdentified {optimal_k} market regimes:\")\n",
    "print(cluster_analysis)\n",
    "print(f\"\\nCluster sizes: {cluster_counts.to_dict()}\")\n",
    "\n",
    "# 2. PCA Visualization\n",
    "print(\"\\n2. PCA VISUALIZATION\")\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(cluster_data_scaled)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "\n",
    "for i in range(optimal_k):\n",
    "    mask = cluster_labels == i\n",
    "    plt.scatter(pca_components[mask, 0], pca_components[mask, 1], \n",
    "               c=colors[i], label=f'Regime {i}', alpha=0.7)\n",
    "\n",
    "plt.xlabel(f'First Principal Component (explained variance: {pca.explained_variance_ratio_[0]:.2%})')\n",
    "plt.ylabel(f'Second Principal Component (explained variance: {pca.explained_variance_ratio_[1]:.2%})')\n",
    "plt.title('Market Regimes in PCA Space')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('outputs/market_regimes_pca.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total explained variance by 2 components: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# 3. Regime-specific Performance Analysis\n",
    "print(\"\\n3. REGIME-SPECIFIC PERFORMANCE ANALYSIS\")\n",
    "\n",
    "regime_performance = feature_data.groupby('market_regime').agg({\n",
    "    'total_pnl': ['mean', 'std', 'min', 'max'],\n",
    "    'daily_win_rate': ['mean', 'std'],\n",
    "    'value': ['mean', 'std'],\n",
    "    'total_volume': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(\"Performance characteristics by market regime:\")\n",
    "print(regime_performance)\n",
    "\n",
    "# Label regimes based on characteristics\n",
    "regime_labels = {\n",
    "    0: 'High Volatility Fear',\n",
    "    1: 'Stable Neutral',\n",
    "    2: 'Greed Rally',\n",
    "    3: 'Extreme Conditions'\n",
    "}\n",
    "\n",
    "# Add descriptive labels\n",
    "feature_data['regime_description'] = feature_data['market_regime'].map(regime_labels)\n",
    "\n",
    "print(\"\\nRegime descriptions:\")\n",
    "for regime, description in regime_labels.items():\n",
    "    regime_data = feature_data[feature_data['market_regime'] == regime]\n",
    "    avg_sentiment = regime_data['value'].mean()\n",
    "    avg_pnl = regime_data['total_pnl'].mean()\n",
    "    print(f\"Regime {regime} ({description}): Avg Sentiment={avg_sentiment:.1f}, Avg PnL=${avg_pnl:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcbd908",
   "metadata": {},
   "source": [
    "## 7. Model Validation and Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f90d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model validation and backtesting\n",
    "print(\"=== MODEL VALIDATION AND BACKTESTING ===\")\n",
    "\n",
    "# 1. Walk-forward validation\n",
    "print(\"\\n1. WALK-FORWARD VALIDATION\")\n",
    "\n",
    "def walk_forward_validation(X, y, model, window_size=30):\n",
    "    \"\"\"\n",
    "    Perform walk-forward validation for time series data\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    for i in range(window_size, len(X)):\n",
    "        # Training window\n",
    "        X_train_window = X.iloc[max(0, i-window_size):i]\n",
    "        y_train_window = y.iloc[max(0, i-window_size):i]\n",
    "        \n",
    "        # Test point\n",
    "        X_test_point = X.iloc[i:i+1]\n",
    "        y_test_point = y.iloc[i]\n",
    "        \n",
    "        # Fit and predict\n",
    "        model.fit(X_train_window, y_train_window)\n",
    "        y_pred_point = model.predict(X_test_point)[0]\n",
    "        \n",
    "        predictions.append(y_pred_point)\n",
    "        actuals.append(y_test_point)\n",
    "    \n",
    "    return np.array(actuals), np.array(predictions)\n",
    "\n",
    "# Perform walk-forward validation\n",
    "rf_model = RandomForestRegressor(n_estimators=50, random_state=42)  # Smaller for speed\n",
    "y_actual_wf, y_pred_wf = walk_forward_validation(X, y, rf_model, window_size=50)\n",
    "\n",
    "# Calculate walk-forward metrics\n",
    "wf_mse = mean_squared_error(y_actual_wf, y_pred_wf)\n",
    "wf_mae = mean_absolute_error(y_actual_wf, y_pred_wf)\n",
    "wf_r2 = r2_score(y_actual_wf, y_pred_wf)\n",
    "\n",
    "print(f\"Walk-Forward Validation Results:\")\n",
    "print(f\"  MSE: {wf_mse:.4f}\")\n",
    "print(f\"  MAE: {wf_mae:.4f}\")\n",
    "print(f\"  R²: {wf_r2:.4f}\")\n",
    "\n",
    "# 2. Trading Strategy Backtesting\n",
    "print(\"\\n2. TRADING STRATEGY BACKTESTING\")\n",
    "\n",
    "def simple_trading_strategy(predictions, actuals, threshold=0):\n",
    "    \"\"\"\n",
    "    Simple strategy: Buy when prediction > threshold, otherwise hold cash\n",
    "    \"\"\"\n",
    "    positions = (predictions > threshold).astype(int)\n",
    "    returns = positions * actuals\n",
    "    \n",
    "    total_return = returns.sum()\n",
    "    avg_return = returns.mean()\n",
    "    win_rate = (returns > 0).mean()\n",
    "    sharpe_ratio = avg_return / returns.std() if returns.std() != 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_return': total_return,\n",
    "        'avg_return': avg_return,\n",
    "        'win_rate': win_rate,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'num_trades': positions.sum()\n",
    "    }\n",
    "\n",
    "# Test different thresholds\n",
    "thresholds = [0, 100, 500, 1000]\n",
    "strategy_results = {}\n",
    "\n",
    "for threshold in thresholds:\n",
    "    results = simple_trading_strategy(y_pred_wf, y_actual_wf, threshold)\n",
    "    strategy_results[threshold] = results\n",
    "    \n",
    "    print(f\"\\nStrategy with threshold ${threshold}:\")\n",
    "    print(f\"  Total Return: ${results['total_return']:,.2f}\")\n",
    "    print(f\"  Average Return: ${results['avg_return']:.2f}\")\n",
    "    print(f\"  Win Rate: {results['win_rate']:.2%}\")\n",
    "    print(f\"  Sharpe Ratio: {results['sharpe_ratio']:.4f}\")\n",
    "    print(f\"  Number of Trades: {results['num_trades']}\")\n",
    "\n",
    "# 3. Sentiment-based Strategy Backtesting\n",
    "print(\"\\n3. SENTIMENT-BASED STRATEGY BACKTESTING\")\n",
    "\n",
    "def sentiment_contrarian_strategy(sentiment_values, actual_returns):\n",
    "    \"\"\"\n",
    "    Contrarian strategy: Buy during extreme fear, sell during extreme greed\n",
    "    \"\"\"\n",
    "    positions = np.zeros(len(sentiment_values))\n",
    "    \n",
    "    # Buy during fear (sentiment < 25), sell during greed (sentiment > 75)\n",
    "    positions[sentiment_values < 25] = 1   # Buy during fear\n",
    "    positions[sentiment_values > 75] = -1  # Short during greed\n",
    "    \n",
    "    returns = positions * actual_returns\n",
    "    \n",
    "    return {\n",
    "        'total_return': returns.sum(),\n",
    "        'avg_return': returns.mean(),\n",
    "        'win_rate': (returns > 0).mean(),\n",
    "        'sharpe_ratio': returns.mean() / returns.std() if returns.std() != 0 else 0,\n",
    "        'positions': positions\n",
    "    }\n",
    "\n",
    "# Apply sentiment strategy to test period\n",
    "test_sentiment = feature_data['value'].iloc[split_point:].values\n",
    "test_returns = feature_data['total_pnl'].iloc[split_point:].values\n",
    "\n",
    "sentiment_strategy_result = sentiment_contrarian_strategy(test_sentiment, test_returns)\n",
    "\n",
    "print(\"Sentiment Contrarian Strategy Results:\")\n",
    "print(f\"  Total Return: ${sentiment_strategy_result['total_return']:,.2f}\")\n",
    "print(f\"  Average Return: ${sentiment_strategy_result['avg_return']:.2f}\")\n",
    "print(f\"  Win Rate: {sentiment_strategy_result['win_rate']:.2%}\")\n",
    "print(f\"  Sharpe Ratio: {sentiment_strategy_result['sharpe_ratio']:.4f}\")\n",
    "\n",
    "# Compare with buy-and-hold\n",
    "buy_hold_return = test_returns.sum()\n",
    "print(f\"\\nBuy-and-Hold Return: ${buy_hold_return:,.2f}\")\n",
    "print(f\"Sentiment Strategy vs Buy-and-Hold: {sentiment_strategy_result['total_return'] / buy_hold_return:.2f}x\")\n",
    "\n",
    "print(\"\\nModel validation and backtesting completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e45a192",
   "metadata": {},
   "source": [
    "## 8. Final Model Summary and Deployment Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4078f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive summary\n",
    "print(\"=== FINAL MODEL SUMMARY AND DEPLOYMENT RECOMMENDATIONS ===\")\n",
    "\n",
    "# 1. Best performing models summary\n",
    "print(\"\\n1. BEST PERFORMING MODELS\")\n",
    "\n",
    "# Regression models comparison\n",
    "print(\"\\nRegression Models (PnL Forecasting):\")\n",
    "for name, results in model_results.items():\n",
    "    print(f\"  {name}: R² = {results['R2']:.4f}, MAE = {results['MAE']:.2f}\")\n",
    "\n",
    "# Classification models comparison\n",
    "print(\"\\nClassification Models (Profitable Day Prediction):\")\n",
    "for name, results in clf_results.items():\n",
    "    print(f\"  {name}: Accuracy = {results['accuracy']:.4f}\")\n",
    "\n",
    "# 2. Key insights from modeling\n",
    "print(\"\\n2. KEY INSIGHTS FROM MODELING\")\n",
    "\n",
    "print(\"\\nMost Predictive Features:\")\n",
    "top_features = feature_importance.head(5)\n",
    "for _, row in top_features.iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\nMarket Regime Insights:\")\n",
    "for regime in range(optimal_k):\n",
    "    regime_data = feature_data[feature_data['market_regime'] == regime]\n",
    "    avg_return = regime_data['total_pnl'].mean()\n",
    "    avg_sentiment = regime_data['value'].mean()\n",
    "    regime_desc = regime_labels.get(regime, f'Regime {regime}')\n",
    "    print(f\"  {regime_desc}: Avg Return = ${avg_return:,.0f}, Avg Sentiment = {avg_sentiment:.1f}\")\n",
    "\n",
    "# 3. Trading strategy recommendations\n",
    "print(\"\\n3. TRADING STRATEGY RECOMMENDATIONS\")\n",
    "\n",
    "print(\"\\nStrategy Performance Comparison:\")\n",
    "print(f\"  Buy-and-Hold Strategy: ${buy_hold_return:,.2f}\")\n",
    "print(f\"  Sentiment Contrarian Strategy: ${sentiment_strategy_result['total_return']:,.2f}\")\n",
    "print(f\"  Performance Ratio: {sentiment_strategy_result['total_return'] / buy_hold_return:.2f}x\")\n",
    "\n",
    "print(\"\\nOptimal Strategy Components:\")\n",
    "print(\"  1. Use sentiment extremes as contrarian signals\")\n",
    "print(\"  2. Monitor lagged sentiment values for momentum\")\n",
    "print(\"  3. Incorporate volume and trade count as confirmation\")\n",
    "print(\"  4. Apply regime-based position sizing\")\n",
    "\n",
    "# 4. Deployment recommendations\n",
    "print(\"\\n4. DEPLOYMENT RECOMMENDATIONS\")\n",
    "\n",
    "print(\"\\nModel Deployment Strategy:\")\n",
    "print(\"  • Primary Model: Random Forest for daily PnL prediction\")\n",
    "print(\"  • Secondary Model: Logistic Regression for trade direction\")\n",
    "print(\"  • Validation: Walk-forward validation with 30-day window\")\n",
    "print(\"  • Retraining: Weekly model updates with new data\")\n",
    "\n",
    "print(\"\\nRisk Management:\")\n",
    "print(\"  • Maximum position size: 2% of portfolio per trade\")\n",
    "print(\"  • Stop-loss: 5% below entry price\")\n",
    "print(\"  • Regime-based scaling: Reduce size during high volatility regimes\")\n",
    "\n",
    "print(\"\\nMonitoring and Alerting:\")\n",
    "print(\"  • Daily model performance tracking\")\n",
    "print(\"  • Sentiment extreme alerts (< 20 or > 80)\")\n",
    "print(\"  • Regime change detection\")\n",
    "print(\"  • Model drift monitoring\")\n",
    "\n",
    "# 5. Save model artifacts\n",
    "print(\"\\n5. SAVING MODEL ARTIFACTS\")\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save best models\n",
    "joblib.dump(models['Random Forest'], 'outputs/rf_pnl_model.pkl')\n",
    "joblib.dump(clf_models['Random Forest Classifier'], 'outputs/rf_classification_model.pkl')\n",
    "joblib.dump(scaler, 'outputs/feature_scaler.pkl')\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('outputs/feature_importance.csv', index=False)\n",
    "clf_feature_importance.to_csv('outputs/classification_feature_importance.csv', index=False)\n",
    "\n",
    "# Save processed data\n",
    "feature_data.to_csv('outputs/processed_modeling_data.csv', index=False)\n",
    "\n",
    "print(\"Model artifacts saved to outputs/ directory\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADVANCED MODELING ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
